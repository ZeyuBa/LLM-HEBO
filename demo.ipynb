{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization with GPT-3.5/4 and HEBO\n",
    "We will start by setting up the environment and necessary configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_PATH:  /mnt/c/study/LMs/LLMs/projects/LLM-HEBO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venido/miniconda3/envs/hpo/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup\n",
    "import os\n",
    "\n",
    "# os.environ['HTTP_PROXY'] = '127.0.0.1:7890'\n",
    "# os.environ['HTTPS_PROXY'] = '127.0.0.1:7890'\n",
    "import datetime\n",
    "import functools\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "from pathlib import Path\n",
    "import json_repair\n",
    "\n",
    "import numpy as np\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from loguru import logger\n",
    "\n",
    "# Load environment variables\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Set ROOT_PATH and other configurations\n",
    "ROOT_PATH = str(Path('.').resolve())\n",
    "print('ROOT_PATH: ', ROOT_PATH)\n",
    "sys.path.insert(0, ROOT_PATH)\n",
    "\n",
    "import prompt_utils\n",
    "\n",
    "_OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "_OPTIMIZER = \"gpt-3.5-turbo\"\n",
    "openai_api_key = _OPENAI_API_KEY\n",
    "\n",
    "if _OPTIMIZER in {\"gpt-3.5-turbo\", \"gpt-4o\"}:\n",
    "    openai.api_key = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Logging\n",
    "We will now configure the optimization parameters and set up logging for debugging and tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization configuration\n",
    "num_points = 50\n",
    "max_num_steps = 3\n",
    "num_reps = 2\n",
    "max_num_pairs = 20\n",
    "num_generated_points_in_each_step = 8\n",
    "\n",
    "# Set the optimizer configurations\n",
    "optimizer_llm_name = _OPTIMIZER\n",
    "optimizer_gpt_max_decode_steps = 1024\n",
    "optimizer_gpt_temperature = 1.0\n",
    "\n",
    "# Create the result directory\n",
    "datetime_str = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "save_folder = os.path.join(ROOT_PATH, \"outputs\", \"optimization-results\", f\"llm_hebo-o-{optimizer_llm_name}-{datetime_str}/\")\n",
    "os.makedirs(save_folder)\n",
    "logger.add(save_folder + \"log.log\", format=\"{time} {level} {message}\", level=\"DEBUG\")\n",
    "print(f\"Result directory:\\n{save_folder}\")\n",
    "\n",
    "# Set optimizer LLM dictionary\n",
    "optimizer_llm_dict = {\n",
    "    \"max_decode_steps\": optimizer_gpt_max_decode_steps,\n",
    "    \"temperature\": optimizer_gpt_temperature,\n",
    "    \"batch_size\": 1\n",
    "}\n",
    "\n",
    "import asyncio\n",
    "\n",
    "call_optimizer_server_func = functools.partial(\n",
    "    prompt_utils.call_openai_server_func,\n",
    "    model=optimizer_llm_name,\n",
    "    max_decode_steps=optimizer_gpt_max_decode_steps,\n",
    "    temperature=optimizer_gpt_temperature,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Optimizer Server\n",
    "Before proceeding with the optimization process, let's test the optimizer server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the optimizer server\n",
    "print(\"\\n======== Testing the optimizer server ===========\")\n",
    "optimizer_test_output = asyncio.run(call_optimizer_server_func(\n",
    "    \"Does the sun rise from the north? Just answer yes or no.\",\n",
    "    temperature=1.0\n",
    "))\n",
    "print(f\"Optimizer test output: {optimizer_test_output}\")\n",
    "print(\"Finished testing the optimizer server.\")\n",
    "print(\"\\n=================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Settings and Initialization\n",
    "Next, we will load the benchmark settings and initialize the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venido/miniconda3/envs/hpo/lib/python3.9/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n",
      "[DEBUG] openml.datasets.dataset at 2024-05-22 15:17:51,598 --- Data pickle file already exists and is up to date.\n",
      "[DEBUG] openml.datasets.dataset at 2024-05-22 15:17:51,603 --- Data pickle file already exists and is up to date.\n",
      "[DEBUG] openml.datasets.dataset at 2024-05-22 15:17:51,642 --- Data pickle file already exists and is up to date.\n",
      "[DEBUG] openml.datasets.dataset at 2024-05-22 15:17:51,655 --- Data pickle file already exists and is up to date.\n",
      "[DEBUG] openml.datasets.dataset at 2024-05-22 15:17:51,658 --- Data pickle file already exists and is up to date.\n",
      "[DEBUG] openml.datasets.dataset at 2024-05-22 15:17:51,666 --- Data pickle file already exists and is up to date.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### TASK 1 of 72: Task-Id: 167149 ###################\n",
      "Hyperparameter default bounds:\n",
      "colsample_bylevel: Lower = 0.01, Upper = 1.0, Check_int = False, Log_sample = False\n",
      "colsample_bytree: Lower = 0.01, Upper = 1.0, Check_int = False, Log_sample = False\n",
      "eta: Lower = -10.0, Upper = 0.0, Check_int = False, Log_sample = True\n",
      "max_depth: Lower = 1, Upper = 15, Check_int = True, Log_sample = False\n",
      "min_child_weight: Lower = 0.0, Upper = 7.0, Check_int = False, Log_sample = True\n",
      "reg_alpha: Lower = -10.0, Upper = 10.0, Check_int = False, Log_sample = True\n",
      "reg_lambda: Lower = -10.0, Upper = 10.0, Check_int = False, Log_sample = True\n",
      "subsample_per_it: Lower = 0.1, Upper = 1.0, Check_int = False, Log_sample = False\n",
      "#################### TASK 2 of 72: Task-Id: 167150 ###################\n",
      "Hyperparameter default bounds:\n",
      "colsample_bylevel: Lower = 0.01, Upper = 1.0, Check_int = False, Log_sample = False\n",
      "colsample_bytree: Lower = 0.01, Upper = 1.0, Check_int = False, Log_sample = False\n",
      "eta: Lower = -10.0, Upper = 0.0, Check_int = False, Log_sample = True\n",
      "max_depth: Lower = 1, Upper = 15, Check_int = True, Log_sample = False\n",
      "min_child_weight: Lower = 0.0, Upper = 7.0, Check_int = False, Log_sample = True\n",
      "reg_alpha: Lower = -10.0, Upper = 10.0, Check_int = False, Log_sample = True\n",
      "reg_lambda: Lower = -10.0, Upper = 10.0, Check_int = False, Log_sample = True\n",
      "subsample_per_it: Lower = 0.1, Upper = 1.0, Check_int = False, Log_sample = False\n"
     ]
    }
   ],
   "source": [
    "from hpobench.util.openml_data_manager import get_openmlcc18_taskids\n",
    "from hpobench.benchmarks.ml.xgboost_benchmark_old import XGBoostBenchmark as Benchmark\n",
    "import time\n",
    "\n",
    "task_ids = get_openmlcc18_taskids()\n",
    "task_no, task_id = 0, task_ids[0]\n",
    "other_info = {}\n",
    "\n",
    "# Initialize tasks and benchmarks\n",
    "for task_no, task_id in enumerate(task_ids[:2]):\n",
    "    print(f'#################### TASK {task_no + 1} of {len(task_ids)}: Task-Id: {task_id} ###################')\n",
    "    benchmark = Benchmark(task_id=task_id)\n",
    "    if benchmark:\n",
    "        start = time.time()\n",
    "        cs = benchmark.get_configuration_space()\n",
    "        results = []\n",
    "        default_bounds = []\n",
    "        print(\"Hyperparameter default bounds:\")\n",
    "        for hyperparameter in list(cs.values()):\n",
    "            name = hyperparameter.name\n",
    "            lower = hyperparameter.lower\n",
    "            upper = hyperparameter.upper\n",
    "            log = hyperparameter.log\n",
    "            check_int = True if \"check_int\" in dir(hyperparameter) else False\n",
    "            if log:\n",
    "                lower = (math.log(lower, 2))\n",
    "                upper = (math.log(upper, 2))\n",
    "            default_bounds.append((lower, upper))\n",
    "            other_info[name] = [log, check_int]\n",
    "            print(f\"{name}: Lower = {lower}, Upper = {upper}, Check_int = {check_int}, Log_sample = {log}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'XGBoostBenchmark'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(type(benchmark).__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "Here, we define the utility functions that will be used throughout the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hebo.optimizers.hebo import HEBO\n",
    "from hebo.design_space.design_space import DesignSpace\n",
    "from tqdm import tqdm\n",
    "from banks import Prompt\n",
    "\n",
    "# Utility functions\n",
    "def evaluate_loss(benchmark, space_list, fidelity):\n",
    "    def preprocess_func(df):\n",
    "        return_dict = {}\n",
    "        for key in df.columns:\n",
    "            value = df[key].iloc[0]\n",
    "            if isinstance(df[key].iloc[0], np.int64):\n",
    "                value = int(df[key].iloc[0])\n",
    "            elif isinstance(df[key].iloc[0], np.float64):\n",
    "                value = float(df[key].iloc[0])\n",
    "            return_dict[key.split('_log')[0]] = 2.0 ** value if '_log' in key else value\n",
    "        return return_dict\n",
    "\n",
    "    def objective(df):\n",
    "        config = preprocess_func(df)\n",
    "        result_dict = benchmark.objective_function(config, fidelity=fidelity)\n",
    "        return result_dict\n",
    "\n",
    "    logger.info(space_list)\n",
    "    sp = DesignSpace().parse(space_list)\n",
    "    opt = HEBO(sp, rand_sample=4)\n",
    "\n",
    "    for i in tqdm(range(10)):\n",
    "        try:\n",
    "            rec = opt.suggest(n_suggestions=5)\n",
    "            result_dict = objective(rec)\n",
    "            valid_loss = result_dict['function_value']\n",
    "            train_loss = result_dict['info']['train_loss']\n",
    "            y = np.array([-valid_loss], dtype=np.float64)\n",
    "            opt.observe(rec, y)\n",
    "        except Exception as e:\n",
    "            logger.debug(e)\n",
    "            continue\n",
    "        logger.info('After %d iterations, best obj is %.2f' % (i, -opt.best_y))\n",
    "    try:\n",
    "        best_config = preprocess_func(opt.best_x)\n",
    "        print(\"Best params is: \", best_config)\n",
    "        result_dict_test = benchmark.objective_function_test(best_config)\n",
    "        test_loss = result_dict_test['function_value']\n",
    "\n",
    "        return {\n",
    "            'configuration': best_config,\n",
    "            'fidelity': fidelity,\n",
    "            'test_loss': -np.round(test_loss, 3),\n",
    "            'valid_loss': valid_loss,\n",
    "            'train_loss': train_loss\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.debug(e)\n",
    "        return {}\n",
    "\n",
    "def gen_meta_prompt(bound_info, test_loss, old_value_pairs_set, max_num_pairs=100):\n",
    "    bound_names = bound_info.keys()\n",
    "    info = tuple((('bound_range', v['bound_range']),\n",
    "                  ('is_log_sample', v['is_log_sample']),\n",
    "                  ('is_int', v['is_int']))\n",
    "                 for v in bound_info.values())\n",
    "    old_value_pairs_set.add((info, test_loss))\n",
    "\n",
    "    old_value_pairs = list(old_value_pairs_set)\n",
    "    old_value_pairs = sorted(old_value_pairs, key=lambda x: -x[1])[-max_num_pairs:]\n",
    "\n",
    "    old_value_pairs_substr = \"\"\n",
    "    for i, pair in enumerate(old_value_pairs):\n",
    "        old_value_pairs_substr += f\"\\nSuggestion {i}: \"\n",
    "        infos, test_loss = pair\n",
    "        for name, info in zip(bound_names, infos):\n",
    "            old_value_pairs_substr += f\"{name} : \"\n",
    "            old_value_pairs_substr += '( ' + ', '.join([f'{key}: {value}' for key, value in info]) + '), '\n",
    "        old_value_pairs_substr += f' test loss: {test_loss}'\n",
    "\n",
    "    meta_prompt = \"\"\"\n",
    "    As an ML engineer, your task is to provide recommended lower and upper bounds for each hyperparameter in the {algo.name} algorithm. You already have reference data on some ranges and the corresponding test loss for these bounds, with the parameter bounds organized in descending order based on their test loss, where lower values indicate better performance. Analyze each hyperparameter to determine reasonable ranges that optimize model performance, ensuring these bounds are grounded in empirical evidence or established best practices. Your insights will be crucial for refining and optimizing the tuning process for {algo.name} models.\n",
    "  Here are some previously suggested ranges and their performance:\n",
    "    \"\"\".strip()\n",
    "    meta_prompt += \"\\n\\n\"\n",
    "    meta_prompt += old_value_pairs_substr.strip()\n",
    "    meta_prompt += \"\\n\\n\"\n",
    "    meta_prompt += \"\"\"\n",
    "    Please provide a new set of recommended lower and upper bounds for each hyperparameter, ensuring that these ranges are different from any previously suggested ranges. Additionally, ensure that the valid loss value associated with these new ranges is lower than any previously mentioned values. Do not write code. \n",
    "  Your output must follow this json format:\n",
    "    \"\"\".strip()\n",
    "    prompt_template = '''\n",
    "  {\n",
    "    {% for hyper_param in hyper_params_l %}\n",
    "    \"{{ hyper_param }}\": {\n",
    "        \"lower_bound\": \"your lower_bound here\",\n",
    "        \"upper_bound\": \"your upper_bound here\"\n",
    "    },\n",
    "\n",
    "    {% endfor %}\n",
    "  }\n",
    "    '''\n",
    "    p=Prompt(prompt_template)\n",
    "    meta_prompt+=p.text({\"hyper_params_l\": list(bound_info.keys())})\n",
    "    meta_prompt+='''\n",
    "where lower_bound and upper_bound are all numerical values. \n",
    "\n",
    "Answer:\n",
    "```json\n",
    "  '''\n",
    "    return meta_prompt\n",
    "\n",
    "def extract_string(input_string):\n",
    "  raw_result=input_string.split('```')[0]\n",
    "  return raw_result\n",
    "\n",
    "def parse_output(extracted_output):\n",
    "\n",
    "  if not extracted_output:\n",
    "    return\n",
    "  bounds = []\n",
    "  try:\n",
    "    bounds_dict=eval(extracted_output)\n",
    "  except:\n",
    "    good_json_string = json_repair.repair_json(extracted_output, skip_json_loads=True)\n",
    "    bounds_dict =json.loads(good_json_string)\n",
    "  for param_name, range in bounds_dict.items():\n",
    "      lower_bound=eval(range['lower_bound']) if isinstance(range['lower_bound'],str) else range['lower_bound']\n",
    "      upper_bound=eval(range['upper_bound']) if isinstance(range['upper_bound'],str) else range['upper_bound']\n",
    "      bounds.append((lower_bound,upper_bound))\n",
    "  return bounds\n",
    "def process_output(bounds,other_info):\n",
    "  space_list=[]\n",
    "  bound_info={}\n",
    "  for hyper_param_info, bound in zip(other_info.items(),bounds):\n",
    "    param_name,param_info=hyper_param_info\n",
    "    if param_info[0]: # is_log_sample\n",
    "      space_dict = {\n",
    "          'name' : param_name+'_log', \n",
    "          'type' : 'int', \n",
    "          'lb' : bound[0], \n",
    "          'ub' : bound[1],\n",
    "          }\n",
    "      bound_info[param_name]={\n",
    "        'bound_range': (bound[0],bound[1]), \"is_log_sample\": param_info[0], \"is_int\": param_info[1]\n",
    "          }\n",
    "    else:\n",
    "      if param_info[1]: # is_int\n",
    "        space_dict = {\n",
    "          'name' : param_name, \n",
    "          'type' : 'int', \n",
    "          'lb' : int(bound[0]),\n",
    "          'ub' : int(bound[1])}\n",
    "        bound_info[param_name]={\n",
    "        'bound_range': (int(bound[0]),int(bound[1])), \"is_log_sample\": param_info[0], \"is_int\": param_info[1]\n",
    "          }\n",
    "\n",
    "      else:\n",
    "        space_dict = {\n",
    "        'name' : param_name, \n",
    "        'type' : 'num', \n",
    "        'lb' : bound[0], \n",
    "        'ub' : bound[1]} \n",
    "        bound_info[param_name]={\n",
    "        'bound_range': (bound[0],bound[1]), \"is_log_sample\": param_info[0], \"is_int\": param_info[1]\n",
    "          }\n",
    "\n",
    "    space_list.append(space_dict)\n",
    "  return bound_info,space_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Optimization Process\n",
    "We will now run the optimization process using the configurations and utility functions defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_dict = dict()\n",
    "results_dict = dict()\n",
    "num_convergence_steps = []\n",
    "\n",
    "for i_rep in range(num_reps):\n",
    "    found_optimal = False\n",
    "    print(f\"\\nRep {i_rep}:\")\n",
    "    \n",
    "    # Generate the starting points\n",
    "    init_bounds = default_bounds\n",
    "    init_fidelity = {'n_estimators': 8, 'dataset_fraction': 0.4}\n",
    "\n",
    "    configs_dict_single_rep = {\n",
    "        \"optimizer_llm_configs\": optimizer_llm_dict,\n",
    "        \"init_bounds\": init_bounds,\n",
    "        \"max_num_steps\": max_num_steps,\n",
    "        \"max_num_pairs\": max_num_pairs,\n",
    "        \"num_generated_points_in_each_step\": num_generated_points_in_each_step,\n",
    "    }\n",
    "    configs_dict[i_rep] = configs_dict_single_rep\n",
    "    configs_json_path = os.path.join(save_folder, \"configs.json\")\n",
    "    print(f\"Saving configs to\\n{configs_json_path}\")\n",
    "    with open(configs_json_path, \"w\") as f:\n",
    "        json.dump(configs_dict, f, indent=4)\n",
    "\n",
    "    old_value_pairs_set = set()\n",
    "    old_value_pairs_with_i_step = []\n",
    "    meta_prompts_dict = dict()\n",
    "    raw_outputs_dict = dict()\n",
    "    init_space_list = []\n",
    "    bound_info, init_space_list = process_output(init_bounds, other_info)\n",
    "    init_test_loss = evaluate_loss(benchmark, init_space_list, init_fidelity)['test_loss']\n",
    "    bound_range = tuple([v['bound_range'] for v in bound_info.values()])\n",
    "    old_value_pairs_with_i_step.append((bound_range, init_test_loss, -1))\n",
    "\n",
    "    print(\"\\n================ Run Optimization ==============\")\n",
    "\n",
    "    results_json_path = os.path.join(save_folder, \"results.json\")\n",
    "    print(f\"Saving results to\\n{results_json_path}\")\n",
    "    test_loss = init_test_loss\n",
    "    for i_step in range(max_num_steps):\n",
    "        print(f\"\\nStep {i_step}:\")\n",
    "        meta_prompt = gen_meta_prompt(bound_info, test_loss, old_value_pairs_set, max_num_pairs=max_num_pairs)\n",
    "\n",
    "        if not i_step % 5:\n",
    "            print(\"\\n=================================================\")\n",
    "        meta_prompts_dict[i_step] = meta_prompt\n",
    "\n",
    "        # Generate points\n",
    "        remaining_num_points_to_generate = num_generated_points_in_each_step\n",
    "        raw_outputs = []\n",
    "        while remaining_num_points_to_generate > 0:\n",
    "            raw_outputs += asyncio.run(call_optimizer_server_func(meta_prompt))\n",
    "            remaining_num_points_to_generate -= optimizer_llm_dict[\"batch_size\"]\n",
    "        raw_outputs = raw_outputs[:num_generated_points_in_each_step]\n",
    "        raw_outputs_dict[i_step] = raw_outputs\n",
    "        parsed_outputs = []\n",
    "        for string in raw_outputs:\n",
    "            try:\n",
    "                parsed_output = parse_output(extract_string(string))\n",
    "                if parsed_output is not None:\n",
    "                    parsed_outputs.append(parsed_output)\n",
    "            except Exception as e:\n",
    "                logger.debug(e, string)\n",
    "        parsed_outputs = [tuple(item) for item in parsed_outputs]\n",
    "        print(f\"Proposed points: {parsed_outputs}\")\n",
    "\n",
    "        single_step_values = []\n",
    "        for parsed_bounds in parsed_outputs:\n",
    "            bound_info, space_list = process_output(parsed_bounds, other_info)\n",
    "            bound_range = tuple([v['bound_range'] for v in bound_info.values()])\n",
    "            loss = evaluate_loss(benchmark, space_list, init_fidelity)\n",
    "            if 'test_loss' in loss:\n",
    "                test_loss=loss['test_loss']\n",
    "                single_step_values.append(test_loss)\n",
    "                bound_names=bound_info.keys()\n",
    "                info=tuple((('bound_range',v['bound_range']),\n",
    "                        ('is_log_sample',v['is_log_sample']),\n",
    "                        ('is_int',v['is_int']))\n",
    "                        for v in bound_info.values())\n",
    "            old_value_pairs_set.add((info, test_loss))\n",
    "            old_value_pairs_with_i_step.append((bound_range, test_loss, i_step))\n",
    "        logger.info(f\"Single step values: {single_step_values}\")\n",
    "\n",
    "        results_dict_single_rep = {\n",
    "            \"meta_prompts\": meta_prompts_dict,\n",
    "            \"raw_outputs\": raw_outputs_dict,\n",
    "            \"old_value_pairs_with_i_step\": old_value_pairs_with_i_step,\n",
    "        }\n",
    "        results_dict[i_rep] = results_dict_single_rep\n",
    "        with open(results_json_path, \"w\") as f:\n",
    "            json.dump(results_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions: \n",
    "1. how to set HEBO Configs?\n",
    "2. how to treat value out of default range?\n",
    "3. how to refine the structure of old_value_pairs_substr rendered in prommpt?\n",
    "4. error occurs when compute GPs like:\n",
    "install from source\n",
    "    - cholesky_cpu: 16 of 16 elements of the torch.Size([4, 4]) tensor are NaN.\n",
    "    - Matrix not positive definite after repeatedly adding jitter up to 1.0e-04."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regret plot (bosteps, best function value)\n",
    "\n",
    "another plot the changes of recommendations\n",
    "\n",
    "the order of suggestions( random, loss value)\n",
    "\n",
    "format the code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.quasirandom import SobolEngine\n",
    "from hebo.design_space.design_space import DesignSpace\n",
    "from hebo.acquisitions.acq import MACE\n",
    "from hebo.acq_optimizers.evolution_optimizer import EvolutionOpt\n",
    "from hebo.optimizers.abstract_optimizer import AbstractOptimizer\n",
    "from typing import Optional\n",
    "\n",
    "class RandomSearch(AbstractOptimizer):\n",
    "    support_parallel_opt = True\n",
    "    support_combinatorial = True\n",
    "    support_contextual = True\n",
    "\n",
    "    def __init__(self, space, rand_sample: Optional[int] = None, scramble_seed: Optional[int] = None):\n",
    "        super().__init__(space)\n",
    "        self.space = space\n",
    "        self.X = pd.DataFrame(columns=self.space.para_names)\n",
    "        self.y = np.zeros((0, 1))\n",
    "        self.rand_sample = 1 + self.space.num_paras if rand_sample is None else max(2, rand_sample)\n",
    "        self.scramble_seed = scramble_seed\n",
    "        self.sobol = SobolEngine(self.space.num_paras, scramble=True, seed=scramble_seed)\n",
    "\n",
    "    def quasi_sample(self, n, fix_input=None):\n",
    "        samp = self.sobol.draw(n)\n",
    "        samp = samp * (self.space.opt_ub - self.space.opt_lb) + self.space.opt_lb\n",
    "        x = samp[:, :self.space.num_numeric]\n",
    "        xe = samp[:, self.space.num_numeric:]\n",
    "        for i, n in enumerate(self.space.numeric_names):\n",
    "            if self.space.paras[n].is_discrete_after_transform:\n",
    "                x[:, i] = x[:, i].round()\n",
    "        df_samp = self.space.inverse_transform(x, xe)\n",
    "        if fix_input is not None:\n",
    "            for k, v in fix_input.items():\n",
    "                df_samp[k] = v\n",
    "        return df_samp\n",
    "\n",
    "    def suggest(self, n_suggestions=1, fix_input=None):\n",
    "        sample = self.quasi_sample(n_suggestions, fix_input)\n",
    "        return sample\n",
    "\n",
    "    def observe(self, X, y):\n",
    "        \n",
    "        valid_id = np.where(np.isfinite(y.reshape(-1)))[0].tolist()\n",
    "        XX = X.iloc[valid_id]\n",
    "        yy = y[valid_id].reshape(-1, 1)\n",
    "        self.X = pd.concat([self.X, XX], axis=0, ignore_index=True)\n",
    "        self.y = np.vstack([self.y, yy])\n",
    "\n",
    "    @property\n",
    "    def best_x(self) -> pd.DataFrame:\n",
    "        if self.X.shape[0] == 0:\n",
    "            raise RuntimeError('No data has been observed!')\n",
    "        else:\n",
    "            return self.X.iloc[[self.y.argmin()]]\n",
    "\n",
    "    @property\n",
    "    def best_y(self) -> float:\n",
    "        if self.X.shape[0] == 0:\n",
    "            raise RuntimeError('No data has been observed!')\n",
    "        else:\n",
    "            return self.y.min()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hebo.design_space.design_space import DesignSpace\n",
    "\n",
    "# Example Usage\n",
    "# Assuming 'space' is already defined as an instance of DesignSpace\n",
    "space = DesignSpace().parse([\n",
    "    {\"name\":\"learning_rate\", \"type\": \"num\", \"lb\": 0.001, \"ub\": 0.1},\n",
    "    {\"name\":\"batch_size\",\"type\": \"int\", \"lb\": 32, \"ub\": 128},\n",
    "    {\"name\":\"num_layers\", \"type\": \"int\", \"lb\": 1, \"ub\": 5},\n",
    "    # Add more hyperparameters as needed\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   learning_rate  batch_size  num_layers\n",
      "0       0.074734          42           5\n",
      "   learning_rate  batch_size  num_layers\n",
      "0       0.047948         101           2\n",
      "   learning_rate  batch_size  num_layers\n",
      "0       0.022922          76           4\n",
      "   learning_rate  batch_size  num_layers\n",
      "0       0.099764         114           3\n",
      "   learning_rate  batch_size  num_layers\n",
      "0       0.083086          57           1\n",
      "   learning_rate  batch_size  num_layers\n",
      "0       0.008564         121           4\n",
      "   learning_rate  batch_size  num_layers\n",
      "0       0.032817          49           2\n",
      "   learning_rate  batch_size  num_layers\n",
      "0        0.05883          80           3\n",
      "   learning_rate  batch_size  num_layers\n",
      "0       0.050813          72           2\n",
      "   learning_rate  batch_size  num_layers\n",
      "0       0.028462         106           3\n"
     ]
    }
   ],
   "source": [
    "# Initialize Random Search Optimizer\n",
    "def evaluate(cfg):\n",
    "    return 0.1\n",
    "opt = RandomSearch(space)\n",
    "\n",
    "# Number of suggestions to generate\n",
    "n_suggestions = 10\n",
    "\n",
    "# Get suggestions\n",
    "# suggestions = random_search.suggest(n_suggestions=n_suggestions)\n",
    "# Evaluate suggestions and observe\n",
    "# Assuming 'evaluate' is a function that takes a configuration and returns a performance score\n",
    "for i in range(10):\n",
    "    rec_x = opt.suggest(n_suggestions=1)\n",
    "    y = np.array([evaluate(rec_x)], dtype=np.float64).reshape(-1, 1)\n",
    "    opt.observe(rec_x, y)\n",
    "    print(rec_x)\n",
    "\n",
    "    # # Print the best configuration found\n",
    "    # print(\"Best configuration found:\")\n",
    "    # print(random_search.best_x)\n",
    "    # print(\"Best score:\", random_search.best_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
